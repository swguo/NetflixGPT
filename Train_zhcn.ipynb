{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8dd1faa-73d5-4e70-8c48-fa240f035fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast, GPT2LMHeadModel, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55be448a-0ebf-44d0-a38d-9d2ab7531636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(21131, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定使用單個 GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "PRETRAINMODEL = \"ckiplab/gpt2-base-chinese\" #\"uer/gpt2-chinese-cluecorpussmall\" #\n",
    "# 初始化 tokenizer 並添加特殊標記\n",
    "try:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(PRETRAINMODEL)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading tokenizer:\", e)\n",
    "    pass\n",
    "# 調整 tokenizer，設置特殊標記\n",
    "tokenizer.add_special_tokens({\"bos_token\": \"<|startoftext|>\", \n",
    "                              \"eos_token\": \"<|endoftext|>\", \n",
    "                              \"sep_token\": \"<|sep|>\"})\n",
    "\n",
    "# 設置 pad_token_id 為 eos_token_id\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# 加載 GPT-2 模型並調整詞彙表大小\n",
    "model = GPT2LMHeadModel.from_pretrained(PRETRAINMODEL)\n",
    "# uer/gpt2-chinese-cluecorpussmall\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32ffa81-6770-4784-8c37-7536e147405f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21131, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=21131, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31eb4e04-2561-4008-a394-7f583febc2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 netflix_titles.csv 並選取 title 和 description 欄位\n",
    "data = pd.read_csv('data/netflix_zhcn.csv',encoding=\"utf_8_sig\")\n",
    "data = data[['title', 'content']]\n",
    "\n",
    "# 將 pandas DataFrame 轉換為 Hugging Face 的 Dataset 並劃分訓練和測試集\n",
    "dataset = Dataset.from_pandas(data)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d346881-3d11-49f7-b83a-686826874adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 train 和 test 資料分別轉換為 pandas DataFrame\n",
    "train_df = train_test_split['train'].to_pandas()\n",
    "test_df = train_test_split['test'].to_pandas()\n",
    "\n",
    "# 將 DataFrame 保存為 CSV 文件\n",
    "train_df.to_csv('data/netflix_train_zhcn.csv', encoding=\"utf_8_sig\", index=False)\n",
    "test_df.to_csv('data/netflix_test_zhcn.csv', encoding=\"utf_8_sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d229aec-9781-4faa-b10b-349d6faa5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 tokenization 函數，處理 input_ids 和 labels\n",
    "def tokenize_function(example):\n",
    "    text = f\"<|startoftext|> 標題:{example['title']} <|sep|>描述:{example['content']}\"\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    input_ids = tokens['input_ids']\n",
    "    \n",
    "    # 確定 <|sep|> 的索引，並設置 labels\n",
    "    sep_token_id = tokenizer.convert_tokens_to_ids(\"<|sep|>\")\n",
    "    if sep_token_id in input_ids:\n",
    "        sep_index = input_ids.index(sep_token_id)\n",
    "    else:\n",
    "        sep_index = -1\n",
    "\n",
    "    # 將 labels 複製自 input_ids，並忽略標題部分和填充部分的損失計算\n",
    "    labels = input_ids.copy()\n",
    "    if sep_index != -1:\n",
    "        for i in range(sep_index + 1):\n",
    "            labels[i] = -100\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    labels = [label if label != pad_token_id else -100 for label in labels]\n",
    "    tokens['labels'] = labels\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159b0ea3-4d26-4a07-9859-895a160e2f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24f5f7a76e241b38332b9c9c7afba50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafc160fbfe049e7a485d98784c4d52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1762 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 應用 tokenization 函數\n",
    "tokenized_datasets = datasets.map(tokenize_function, remove_columns=[\"title\", \"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d929bf-36c2-40d0-b809-ce3092a87ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1\n",
      "})\n",
      "inputs:\n",
      "[CLS] <|startoftext|> 標 題 : 掃 帚 上 的 小 房 間 <|sep|> 描 述 : 一 位 溫 和 的 女 巫 用 她 的 紅 髮 辮 子 為 各 種 動 物 提 供 搭 乘 ， 讓 她 脾 氣 暴 躁 的 貓 感 到 非 常 惱 怒 。 [SEP] <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "labels:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2989, 6835, 131, 671, 855, 3984, 1469, 4638, 1957, 2344, 4500, 1961, 4638, 5148, 7773, 6799, 2094, 4158, 1392, 4934, 1240, 4289, 2990, 897, 3022, 733, 8024, 6366, 1961, 5569, 3706, 3274, 6708, 4638, 6506, 2697, 1168, 7478, 2382, 2681, 2584, 511, 102, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "描 述 : 一 位 溫 和 的 女 巫 用 她 的 紅 髮 辮 子 為 各 種 動 物 提 供 搭 乘 ， 讓 她 脾 氣 暴 躁 的 貓 感 到 非 常 惱 怒 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 或者查看隨機一筆資料\n",
    "random_example = tokenized_datasets['train'].shuffle(seed=100).select([0])\n",
    "print(random_example)\n",
    "generated_text = tokenizer.decode(random_example['input_ids'][0], skip_special_tokens=False)\n",
    "print('inputs:')\n",
    "print(generated_text)\n",
    "print('labels:')\n",
    "print(random_example['labels'][0])\n",
    "generated_text = tokenizer.decode([token for token in random_example['labels'][0] if token != -100], skip_special_tokens=False)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb8d037-a5fa-412c-8d1d-b7f7dec4385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 訓練參數設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./NetflixGPT-chinese\",\n",
    "    evaluation_strategy=\"steps\",        # 每隔一定步數進行評估\n",
    "    eval_steps=500,                     # 每200步評估一次\n",
    "    save_steps=500,                     # 每200步保存一次\n",
    "    load_best_model_at_end=True,        # 在訓練結束時加載最佳模型\n",
    "    metric_for_best_model=\"eval_loss\",  # 使用驗證損失作為早停和選擇最佳模型的依據\n",
    "    greater_is_better=False,            # 對於損失，越低越好\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    save_total_limit=1,                 # 僅保留一個最優 checkpoint\n",
    ")\n",
    "\n",
    "# 初始化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # 當指標在 3 次評估步驟中無改善時早停\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d98d5e5-f0ff-4855-9b8b-da9f42a3c5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mswguo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/user_data/workspace/NetflixGPT/wandb/run-20241114_180235-q8fzy3sy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/swguo/huggingface/runs/q8fzy3sy' target=\"_blank\">fast-glitter-124</a></strong> to <a href='https://wandb.ai/swguo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/swguo/huggingface' target=\"_blank\">https://wandb.ai/swguo/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/swguo/huggingface/runs/q8fzy3sy' target=\"_blank\">https://wandb.ai/swguo/huggingface/runs/q8fzy3sy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='44100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3500/44100 08:09 < 1:34:46, 7.14 it/s, Epoch 7/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.833500</td>\n",
       "      <td>2.732565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.431500</td>\n",
       "      <td>2.720229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.137100</td>\n",
       "      <td>2.753879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.872900</td>\n",
       "      <td>2.808597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.624700</td>\n",
       "      <td>2.867477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.389900</td>\n",
       "      <td>2.948426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.177200</td>\n",
       "      <td>3.024760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./NetflixGPT-chinese/checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=1.9238209402901785, metrics={'train_runtime': 492.3703, 'train_samples_per_second': 1430.834, 'train_steps_per_second': 89.567, 'total_flos': 3653058576384000.0, 'train_loss': 1.9238209402901785, 'epoch': 7.94})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 開始訓練\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a17b9607-293f-4947-9e9c-076aae54a8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./NetflixGPT-chinese/tokenizer_config.json',\n",
       " './NetflixGPT-chinese/special_tokens_map.json',\n",
       " './NetflixGPT-chinese/vocab.txt',\n",
       " './NetflixGPT-chinese/added_tokens.json',\n",
       " './NetflixGPT-chinese/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假設模型和 tokenizer 的保存目錄為 \"./gpt2-netflix\"\n",
    "tokenizer.save_pretrained(\"./NetflixGPT-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8f5cda-087c-443e-8bf2-aed50d831045",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8077582-8bd7-4225-b501-fd9327f4ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: 精神病特工\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/envs/LLMs/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Description: 特·格里爾斯和他的朋友們在一個小鎮上度過了一年的假期，他們的生活在他最好的時刻裡面臨著一些令人毛骨悚然的事情。\n",
      "--------------------------------------------------\n",
      "Title: 追星女孩\n",
      "Generated Description: 在一個小鎮上，一位年輕的女子在她的家鄉度過了一年的假期，她在那裡遇到了兩個女人，他們都在尋找自己的方法，並在這個時候遇見了他。\n",
      "--------------------------------------------------\n",
      "Title: 牛奶之水\n",
      "Generated Description: 是一個被遺忘的小鎮，一位年輕的牧場工人和一名年邁的女性在一起，他們在這個充滿活力的故事中找到了一種新的感覺，並學習了他的人生。\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n"
     ]
    }
   ],
   "source": [
    "# 定義 inference 測試函數\n",
    "def generate_description(title):\n",
    "    input_text = f\"標題:{title} 描述:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 使用模型進行生成\n",
    "    output = model.generate(input_ids, max_length=128, num_return_sequences=1, no_repeat_ngram_size=2, \n",
    "                            pad_token_id=tokenizer.pad_token_id, early_stopping=True)\n",
    "    \n",
    "    # 解碼生成的描述\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    #print(generated_text)\n",
    "    generated_text = ''.join(generated_text.split(' '))\n",
    "    new_input_text = ''.join(input_text.split(' '))\n",
    "    return generated_text.replace(new_input_text, \"\").strip()\n",
    "\n",
    "# 測試生成效果\n",
    "test_titles = [\"精神病特工\", \"追星女孩\", \"牛奶之水\"]\n",
    "for title in test_titles:\n",
    "    print(f\"Title: {title}\")\n",
    "    print(\"Generated Description:\", generate_description(title))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c2377-3eef-4d53-a8b7-0be29a8f72af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cb3bb-bf70-4935-9df0-da770195ae8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
