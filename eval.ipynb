{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5757c715-2945-40ac-b8d6-1b89d9eecc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_metric\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cda4e3a-ed47-4ffd-b3fb-fbff1384e4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 載入訓練好的模型和 tokenizer\n",
    "model_path = \"./NetflixGPT-english\"  # 修改為你訓練模型的儲存路徑\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf43719-0453-4ae9-aee7-2aa12b81ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定義生成描述的函數\n",
    "def generate_description(title, max_length=100):\n",
    "    input_text = f\"<|startoftext|>Title: {title}<|sep|>Description:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text.replace(input_text, \"\").strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a096475-87d7-4e5c-bd2b-854a2f608450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加載測試數據集\n",
    "data = pd.read_csv('data/netflix_test_en.csv')\n",
    "data = data[['title', 'description']]\n",
    "test_data = Dataset.from_pandas(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50a4049-1008-4e15-bac7-470db59dc6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test dataset:   0%|          | 0/1762 [00:00<?, ?it/s]/user_data/envs/LLMs/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Evaluating on test dataset: 100%|██████████| 1762/1762 [05:37<00:00,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU-1 Score: 0.12813252341268005\n",
      "Average BLEU-2 Score: 0.017049738103892587\n",
      "Average BLEU-3 Score: 0.002921133683896236\n",
      "Average BLEU-4 Score: 0.000998687316397085\n",
      "Average ROUGE-1 Score: 0.17700172676113987\n",
      "Average ROUGE-2 Score: 0.017623737580960617\n",
      "Average ROUGE-L Score: 0.13143432925502094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 初始化 BLEU 和 ROUGE 評分器\n",
    "bleu_scores = { \"BLEU-1\": [], \"BLEU-2\": [], \"BLEU-3\": [], \"BLEU-4\": [] }\n",
    "rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "# 計算 BLEU 和 ROUGE 分數\n",
    "for example in tqdm(test_data, desc=\"Evaluating on test dataset\"):\n",
    "    title = example['title']\n",
    "    reference = example['description']\n",
    "    prediction = generate_description(title)\n",
    "\n",
    "    # 將參考文本和預測文本分詞\n",
    "    reference_tokens = [reference.split()]\n",
    "    prediction_tokens = prediction.split()\n",
    "    \n",
    "    # 計算 BLEU-1 到 BLEU-4 分數\n",
    "    bleu_scores[\"BLEU-1\"].append(sentence_bleu(reference_tokens, prediction_tokens, weights=(1, 0, 0, 0)))\n",
    "    bleu_scores[\"BLEU-2\"].append(sentence_bleu(reference_tokens, prediction_tokens, weights=(0.5, 0.5, 0, 0)))\n",
    "    bleu_scores[\"BLEU-3\"].append(sentence_bleu(reference_tokens, prediction_tokens, weights=(0.33, 0.33, 0.33, 0)))\n",
    "    bleu_scores[\"BLEU-4\"].append(sentence_bleu(reference_tokens, prediction_tokens, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "    # 計算 ROUGE 分數\n",
    "    rouge_result = rouge_scorer_instance.score(reference, prediction)\n",
    "    rouge_scores['rouge1'].append(rouge_result['rouge1'].fmeasure)\n",
    "    rouge_scores['rouge2'].append(rouge_result['rouge2'].fmeasure)\n",
    "    rouge_scores['rougeL'].append(rouge_result['rougeL'].fmeasure)\n",
    "\n",
    "# 計算每個 BLEU 和 ROUGE 分數的平均值\n",
    "average_bleu1 = sum(bleu_scores[\"BLEU-1\"]) / len(bleu_scores[\"BLEU-1\"])\n",
    "average_bleu2 = sum(bleu_scores[\"BLEU-2\"]) / len(bleu_scores[\"BLEU-2\"])\n",
    "average_bleu3 = sum(bleu_scores[\"BLEU-3\"]) / len(bleu_scores[\"BLEU-3\"])\n",
    "average_bleu4 = sum(bleu_scores[\"BLEU-4\"]) / len(bleu_scores[\"BLEU-4\"])\n",
    "\n",
    "average_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1'])\n",
    "average_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2'])\n",
    "average_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL'])\n",
    "\n",
    "# 輸出結果\n",
    "print(\"Average BLEU-1 Score:\", average_bleu1)\n",
    "print(\"Average BLEU-2 Score:\", average_bleu2)\n",
    "print(\"Average BLEU-3 Score:\", average_bleu3)\n",
    "print(\"Average BLEU-4 Score:\", average_bleu4)\n",
    "\n",
    "print(\"Average ROUGE-1 Score:\", average_rouge1)\n",
    "print(\"Average ROUGE-2 Score:\", average_rouge2)\n",
    "print(\"Average ROUGE-L Score:\", average_rougeL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc71befe-7218-4089-bdca-4bf9ffa98212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
