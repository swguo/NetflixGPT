{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147d7a0c-ee2a-4cd4-ba16-b225e79c5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918594ba-fcd8-4997-b934-c9954d8f8e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Map: 100%|█████████████████████████| 7045/7045 [00:02<00:00, 2511.68 examples/s]\n",
      "Map: 100%|█████████████████████████| 1762/1762 [00:00<00:00, 2568.44 examples/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mswguo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/user_data/workspace/NetflixGPT/wandb/run-20241124_034244-vx36kwk5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-water-127\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/swguo/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/swguo/huggingface/runs/vx36kwk5\u001b[0m\n",
      "  0%|                                                 | 0/11050 [00:00<?, ?it/s]/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 2.6548, 'grad_norm': 2.7705507278442383, 'learning_rate': 4.7737556561085976e-05, 'epoch': 2.26}\n",
      "  5%|█▊                                     | 500/11050 [01:42<36:09,  4.86it/s]\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 3/56 [00:00<00:02, 19.94it/s]\u001b[A\n",
      "  9%|███▉                                        | 5/56 [00:00<00:03, 15.91it/s]\u001b[A\n",
      " 12%|█████▌                                      | 7/56 [00:00<00:03, 14.59it/s]\u001b[A\n",
      " 16%|███████                                     | 9/56 [00:00<00:03, 13.99it/s]\u001b[A\n",
      " 20%|████████▍                                  | 11/56 [00:00<00:03, 13.65it/s]\u001b[A\n",
      " 23%|█████████▉                                 | 13/56 [00:00<00:03, 13.46it/s]\u001b[A\n",
      " 27%|███████████▌                               | 15/56 [00:01<00:03, 13.33it/s]\u001b[A\n",
      " 30%|█████████████                              | 17/56 [00:01<00:02, 13.26it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 19/56 [00:01<00:02, 13.19it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 21/56 [00:01<00:02, 13.17it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 23/56 [00:01<00:02, 13.15it/s]\u001b[A\n",
      " 45%|███████████████████▏                       | 25/56 [00:01<00:02, 13.12it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 27/56 [00:01<00:02, 13.11it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 29/56 [00:02<00:02, 13.11it/s]\u001b[A\n",
      " 55%|███████████████████████▊                   | 31/56 [00:02<00:01, 13.12it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 33/56 [00:02<00:01, 13.08it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 35/56 [00:02<00:01, 13.08it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 37/56 [00:02<00:01, 13.09it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 39/56 [00:02<00:01, 13.08it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 41/56 [00:03<00:01, 13.09it/s]\u001b[A\n",
      " 77%|█████████████████████████████████          | 43/56 [00:03<00:00, 13.08it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▌        | 45/56 [00:03<00:00, 13.08it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 47/56 [00:03<00:00, 13.09it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 49/56 [00:03<00:00, 13.08it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 51/56 [00:03<00:00, 13.08it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 53/56 [00:03<00:00, 13.07it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.7039194107055664, 'eval_runtime': 4.2248, 'eval_samples_per_second': 417.065, 'eval_steps_per_second': 13.255, 'epoch': 2.26}\n",
      "  5%|█▊                                     | 500/11050 [01:46<36:09,  4.86it/s]\n",
      "100%|███████████████████████████████████████████| 56/56 [00:04<00:00, 13.21it/s]\u001b[A\n",
      "                                                                                \u001b[A/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 2.1228, 'grad_norm': 3.111638307571411, 'learning_rate': 4.547511312217195e-05, 'epoch': 4.52}\n",
      "  9%|███▍                                  | 1000/11050 [03:31<34:54,  4.80it/s]\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 3/56 [00:00<00:02, 19.72it/s]\u001b[A\n",
      "  9%|███▉                                        | 5/56 [00:00<00:03, 15.66it/s]\u001b[A\n",
      " 12%|█████▌                                      | 7/56 [00:00<00:03, 14.45it/s]\u001b[A\n",
      " 16%|███████                                     | 9/56 [00:00<00:03, 13.85it/s]\u001b[A\n",
      " 20%|████████▍                                  | 11/56 [00:00<00:03, 13.49it/s]\u001b[A\n",
      " 23%|█████████▉                                 | 13/56 [00:00<00:03, 13.34it/s]\u001b[A\n",
      " 27%|███████████▌                               | 15/56 [00:01<00:03, 13.17it/s]\u001b[A\n",
      " 30%|█████████████                              | 17/56 [00:01<00:02, 13.12it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 19/56 [00:01<00:02, 13.06it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 21/56 [00:01<00:02, 12.97it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 23/56 [00:01<00:02, 13.00it/s]\u001b[A\n",
      " 45%|███████████████████▏                       | 25/56 [00:01<00:02, 12.97it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 27/56 [00:02<00:02, 12.96it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 29/56 [00:02<00:02, 12.97it/s]\u001b[A\n",
      " 55%|███████████████████████▊                   | 31/56 [00:02<00:01, 12.95it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 33/56 [00:02<00:01, 12.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 35/56 [00:02<00:01, 12.93it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 37/56 [00:02<00:01, 12.89it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 39/56 [00:02<00:01, 12.95it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 41/56 [00:03<00:01, 12.94it/s]\u001b[A\n",
      " 77%|█████████████████████████████████          | 43/56 [00:03<00:01, 12.94it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▌        | 45/56 [00:03<00:00, 12.93it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 47/56 [00:03<00:00, 12.92it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 49/56 [00:03<00:00, 12.94it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 51/56 [00:03<00:00, 12.92it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 53/56 [00:04<00:00, 12.93it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.7535340785980225, 'eval_runtime': 4.2726, 'eval_samples_per_second': 412.391, 'eval_steps_per_second': 13.107, 'epoch': 4.52}\n",
      "  9%|███▍                                  | 1000/11050 [03:35<34:54,  4.80it/s]\n",
      "100%|███████████████████████████████████████████| 56/56 [00:04<00:00, 13.04it/s]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./NetflixGPT-chinese-2/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.7246, 'grad_norm': 3.4814329147338867, 'learning_rate': 4.321266968325792e-05, 'epoch': 6.79}\n",
      " 14%|█████▏                                | 1500/11050 [05:26<33:22,  4.77it/s]\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 3/56 [00:00<00:02, 19.62it/s]\u001b[A\n",
      "  9%|███▉                                        | 5/56 [00:00<00:03, 15.64it/s]\u001b[A\n",
      " 12%|█████▌                                      | 7/56 [00:00<00:03, 14.40it/s]\u001b[A\n",
      " 16%|███████                                     | 9/56 [00:00<00:03, 13.76it/s]\u001b[A\n",
      " 20%|████████▍                                  | 11/56 [00:00<00:03, 13.42it/s]\u001b[A\n",
      " 23%|█████████▉                                 | 13/56 [00:00<00:03, 13.26it/s]\u001b[A\n",
      " 27%|███████████▌                               | 15/56 [00:01<00:03, 13.11it/s]\u001b[A\n",
      " 30%|█████████████                              | 17/56 [00:01<00:02, 13.04it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 19/56 [00:01<00:02, 13.00it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 21/56 [00:01<00:02, 12.95it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 23/56 [00:01<00:02, 12.89it/s]\u001b[A\n",
      " 45%|███████████████████▏                       | 25/56 [00:01<00:02, 12.89it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 27/56 [00:02<00:02, 12.90it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 29/56 [00:02<00:02, 12.88it/s]\u001b[A\n",
      " 55%|███████████████████████▊                   | 31/56 [00:02<00:01, 12.91it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 33/56 [00:02<00:01, 12.88it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 35/56 [00:02<00:01, 12.87it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 37/56 [00:02<00:01, 12.88it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 39/56 [00:02<00:01, 12.84it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 41/56 [00:03<00:01, 12.86it/s]\u001b[A\n",
      " 77%|█████████████████████████████████          | 43/56 [00:03<00:01, 12.88it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▌        | 45/56 [00:03<00:00, 12.88it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 47/56 [00:03<00:00, 12.85it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 49/56 [00:03<00:00, 12.86it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 51/56 [00:03<00:00, 12.85it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 53/56 [00:04<00:00, 12.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.844630718231201, 'eval_runtime': 4.293, 'eval_samples_per_second': 410.44, 'eval_steps_per_second': 13.045, 'epoch': 6.79}\n",
      " 14%|█████▏                                | 1500/11050 [05:30<33:22,  4.77it/s]\n",
      "100%|███████████████████████████████████████████| 56/56 [00:04<00:00, 13.01it/s]\u001b[A\n",
      "                                                                                \u001b[A/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.3735, 'grad_norm': 3.3454651832580566, 'learning_rate': 4.095022624434389e-05, 'epoch': 9.05}\n",
      " 18%|██████▉                               | 2000/11050 [07:16<31:29,  4.79it/s]\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 3/56 [00:00<00:02, 19.66it/s]\u001b[A\n",
      "  9%|███▉                                        | 5/56 [00:00<00:03, 15.55it/s]\u001b[A\n",
      " 12%|█████▌                                      | 7/56 [00:00<00:03, 14.31it/s]\u001b[A\n",
      " 16%|███████                                     | 9/56 [00:00<00:03, 13.76it/s]\u001b[A\n",
      " 20%|████████▍                                  | 11/56 [00:00<00:03, 13.38it/s]\u001b[A\n",
      " 23%|█████████▉                                 | 13/56 [00:00<00:03, 13.22it/s]\u001b[A\n",
      " 27%|███████████▌                               | 15/56 [00:01<00:03, 13.10it/s]\u001b[A\n",
      " 30%|█████████████                              | 17/56 [00:01<00:02, 13.00it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 19/56 [00:01<00:02, 12.96it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 21/56 [00:01<00:02, 12.94it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 23/56 [00:01<00:02, 12.89it/s]\u001b[A\n",
      " 45%|███████████████████▏                       | 25/56 [00:01<00:02, 12.84it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 27/56 [00:02<00:02, 12.88it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 29/56 [00:02<00:02, 12.87it/s]\u001b[A\n",
      " 55%|███████████████████████▊                   | 31/56 [00:02<00:01, 12.88it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 33/56 [00:02<00:01, 12.86it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 35/56 [00:02<00:01, 12.83it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 37/56 [00:02<00:01, 12.83it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 39/56 [00:02<00:01, 12.85it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 41/56 [00:03<00:01, 12.84it/s]\u001b[A\n",
      " 77%|█████████████████████████████████          | 43/56 [00:03<00:01, 12.84it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▌        | 45/56 [00:03<00:00, 12.83it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 47/56 [00:03<00:00, 12.82it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 49/56 [00:03<00:00, 12.82it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 51/56 [00:03<00:00, 12.83it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 53/56 [00:04<00:00, 12.82it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.96921706199646, 'eval_runtime': 4.3047, 'eval_samples_per_second': 409.324, 'eval_steps_per_second': 13.009, 'epoch': 9.05}\n",
      " 18%|██████▉                               | 2000/11050 [07:20<31:29,  4.79it/s]\n",
      "100%|███████████████████████████████████████████| 56/56 [00:04<00:00, 12.95it/s]\u001b[A\n",
      "                                                                                \u001b[A/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 1.0565, 'grad_norm': 3.440293550491333, 'learning_rate': 3.868778280542987e-05, 'epoch': 11.31}\n",
      " 23%|████████▌                             | 2500/11050 [09:07<30:00,  4.75it/s]\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 3/56 [00:00<00:02, 19.47it/s]\u001b[A\n",
      "  9%|███▉                                        | 5/56 [00:00<00:03, 15.58it/s]\u001b[A\n",
      " 12%|█████▌                                      | 7/56 [00:00<00:03, 14.30it/s]\u001b[A\n",
      " 16%|███████                                     | 9/56 [00:00<00:03, 13.71it/s]\u001b[A\n",
      " 20%|████████▍                                  | 11/56 [00:00<00:03, 13.39it/s]\u001b[A\n",
      " 23%|█████████▉                                 | 13/56 [00:00<00:03, 13.20it/s]\u001b[A\n",
      " 27%|███████████▌                               | 15/56 [00:01<00:03, 13.08it/s]\u001b[A\n",
      " 30%|█████████████                              | 17/56 [00:01<00:02, 13.00it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 19/56 [00:01<00:02, 12.97it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 21/56 [00:01<00:02, 12.93it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 23/56 [00:01<00:02, 12.90it/s]\u001b[A\n",
      " 45%|███████████████████▏                       | 25/56 [00:01<00:02, 12.89it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 27/56 [00:02<00:02, 12.85it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 29/56 [00:02<00:02, 12.85it/s]\u001b[A\n",
      " 55%|███████████████████████▊                   | 31/56 [00:02<00:01, 12.85it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 33/56 [00:02<00:01, 12.87it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 35/56 [00:02<00:01, 12.82it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 37/56 [00:02<00:01, 12.85it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 39/56 [00:02<00:01, 12.85it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 41/56 [00:03<00:01, 12.82it/s]\u001b[A\n",
      " 77%|█████████████████████████████████          | 43/56 [00:03<00:01, 12.82it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▌        | 45/56 [00:03<00:00, 12.86it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 47/56 [00:03<00:00, 12.82it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 49/56 [00:03<00:00, 12.84it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 51/56 [00:03<00:00, 12.84it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 53/56 [00:04<00:00, 12.83it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.092048406600952, 'eval_runtime': 4.3066, 'eval_samples_per_second': 409.139, 'eval_steps_per_second': 13.003, 'epoch': 11.31}\n",
      " 23%|████████▌                             | 2500/11050 [09:11<30:00,  4.75it/s]\n",
      "100%|███████████████████████████████████████████| 56/56 [00:04<00:00, 12.95it/s]\u001b[A\n",
      "                                                                                \u001b[A/user_data/envs/LLMs/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "{'loss': 0.8097, 'grad_norm': 3.5420074462890625, 'learning_rate': 3.642533936651584e-05, 'epoch': 13.57}\n",
      " 27%|██████████▎                           | 3000/11050 [10:57<28:14,  4.75it/s]\n",
      "  0%|                                                    | 0/56 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 3/56 [00:00<00:02, 19.60it/s]\u001b[A\n",
      "  9%|███▉                                        | 5/56 [00:00<00:03, 15.55it/s]\u001b[A\n",
      " 12%|█████▌                                      | 7/56 [00:00<00:03, 14.29it/s]\u001b[A\n",
      " 16%|███████                                     | 9/56 [00:00<00:03, 13.73it/s]\u001b[A\n",
      " 20%|████████▍                                  | 11/56 [00:00<00:03, 13.38it/s]\u001b[A\n",
      " 23%|█████████▉                                 | 13/56 [00:00<00:03, 13.21it/s]\u001b[A\n",
      " 27%|███████████▌                               | 15/56 [00:01<00:03, 13.10it/s]\u001b[A\n",
      " 30%|█████████████                              | 17/56 [00:01<00:02, 13.00it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 19/56 [00:01<00:02, 12.93it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 21/56 [00:01<00:02, 12.90it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 23/56 [00:01<00:02, 12.88it/s]\u001b[A\n",
      " 45%|███████████████████▏                       | 25/56 [00:01<00:02, 12.86it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 27/56 [00:02<00:02, 12.87it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 29/56 [00:02<00:02, 12.84it/s]\u001b[A\n",
      " 55%|███████████████████████▊                   | 31/56 [00:02<00:01, 12.79it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 33/56 [00:02<00:01, 12.83it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 35/56 [00:02<00:01, 12.83it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 37/56 [00:02<00:01, 12.83it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 39/56 [00:02<00:01, 12.84it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 41/56 [00:03<00:01, 12.83it/s]\u001b[A\n",
      " 77%|█████████████████████████████████          | 43/56 [00:03<00:01, 12.83it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▌        | 45/56 [00:03<00:00, 12.81it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 47/56 [00:03<00:00, 12.82it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 49/56 [00:03<00:00, 12.80it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 51/56 [00:03<00:00, 12.83it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 53/56 [00:04<00:00, 12.85it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.223973035812378, 'eval_runtime': 4.309, 'eval_samples_per_second': 408.916, 'eval_steps_per_second': 12.996, 'epoch': 13.57}\n",
      " 27%|██████████▎                           | 3000/11050 [11:02<28:14,  4.75it/s]\n",
      "100%|███████████████████████████████████████████| 56/56 [00:04<00:00, 12.95it/s]\u001b[A\n",
      "                                                                                \u001b[AThere were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "{'train_runtime': 665.1407, 'train_samples_per_second': 529.587, 'train_steps_per_second': 16.613, 'train_loss': 1.62364306640625, 'epoch': 13.57}\n",
      " 27%|██████████▎                           | 3000/11050 [11:03<29:40,  4.52it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ▁▂▃▅▆█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁▅▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second █▄▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second █▄▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▂▄▄▅▅▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▂▄▄▅▅▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/grad_norm ▁▄▇▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▅▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▆▄▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 3.22397\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 4.309\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 408.916\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 12.996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 13.57\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 3000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/grad_norm 3.54201\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 4e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.8097\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 6248080392192000.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.62364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 665.1407\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 529.587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 16.613\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mefficient-water-127\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/swguo/huggingface/runs/vx36kwk5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241124_034244-vx36kwk5/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} main.py --data_path 'data/netflix_zhcn.csv' \\\n",
    "--output_dir './NetflixGPT-chinese' \\\n",
    "--num_train_epochs 50 \\\n",
    "--batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3ff1d-7477-4863-89f8-742dc0c9a5d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "  0%|                                                   | 0/500 [00:00<?, ?it/s]/user_data/envs/LLMs/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 500/500 [03:01<00:00,  2.75it/s]\n",
      "                 title  ...                              generated_description\n",
      "0           開心鬧翻天：驚心動魄  ...  的女演員兼喜劇演者兼詞曲作家兼歌手兼音樂家伊藤忠夫在這部以女性為主題的紀錄片中，談及她在一個...\n",
      "1             有哭鬧的孩子嗎？  ...  的女兒在一個充滿悲傷的家庭聚會中，面對著一位有抱負的演員，她的母親和一名女性的生活在她們的婚...\n",
      "2                  鐵娘子  ...  年，一位年輕女子在一個神秘的世界中，與一名神父的女兒建立了意想不到的連結，但當她的父親被謀殺...\n",
      "3              我們創造的世界  ...  的創意人士在這部紀錄片中，探討了一些令人驚嘆的事物，包括一個在一次大膽的公路旅行中遇到的女人...\n",
      "4  肯·伯恩斯：羅斯福家族：一段親密的歷史  ...  肯伯·斯的第一部紀錄片系列探討了肯尼·威爾遜的生平故事，他在這部以肯雅家庭為中心的喜劇特輯中...\n",
      "\n",
      "[5 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} inference.py --test_path 'data/netflix_test_zhcn.csv' \\\n",
    "--model_path './NetflixGPT-chinese' \\\n",
    "--output_path 'infr_result.csv' \\\n",
    "--num 500 \\\n",
    "--strategy 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaccd05b-17b0-4611-9cfc-529835024f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                       | 0/500 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.332 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.332 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/user_data/envs/LLMs/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Evaluating: 100%|████████████████████████████| 500/500 [00:01<00:00, 482.33it/s]\n",
      "Evaluation for ZH sentences:\n",
      "Average BLEU Scores: {'BLEU-1': 0.20757570667624003, 'BLEU-2': 0.032062402079734514, 'BLEU-3': 0.004664731809502818, 'BLEU-4': 0.0008132079643434773}\n",
      "Average ROUGE Scores: {'rouge1': 0.009742857142857143, 'rouge2': 0.0021333333333333334, 'rougeL': 0.009742857142857143}\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} eval.py --generated_path 'infr_result.csv' \\\n",
    "--test_path 'data/netflix_test_zhcn.csv' \\\n",
    "--num 500 \\\n",
    "--lang 'zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "192dab06-380f-4149-a4d3-a67b87bf0b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nltk in /user_data/envs/LLMs/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: rouge-score in /user_data/envs/LLMs/lib/python3.8/site-packages (0.1.2)\n",
      "Requirement already satisfied: click in /user_data/envs/LLMs/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /user_data/envs/LLMs/lib/python3.8/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /user_data/envs/LLMs/lib/python3.8/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /user_data/envs/LLMs/lib/python3.8/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: absl-py in /user_data/envs/LLMs/lib/python3.8/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: numpy in /user_data/envs/LLMs/lib/python3.8/site-packages (from rouge-score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /user_data/envs/LLMs/lib/python3.8/site-packages (from rouge-score) (1.16.0)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=d6f616a8d438a48575bd3b8d5bb0875234b95a4882da3d9ea8ec350f17023aed\n",
      "  Stored in directory: /user_data/.cache/pip/wheels/ca/38/d8/dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\n",
      "Successfully built jieba\n",
      "\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/user_data/envs/LLMs/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install jieba nltk rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47818a-9654-411d-ba08-fc2a5f5ec33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
